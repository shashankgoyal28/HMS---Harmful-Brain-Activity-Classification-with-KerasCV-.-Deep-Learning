{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":936546,"sourceType":"datasetVersion","datasetId":504915},{"sourceId":4961534,"sourceType":"datasetVersion","datasetId":2877536},{"sourceId":7526248,"sourceType":"datasetVersion","datasetId":4308295},{"sourceId":6080,"sourceType":"modelInstanceVersion","modelInstanceId":4592},{"sourceId":6098,"sourceType":"modelInstanceVersion","modelInstanceId":4629},{"sourceId":6099,"sourceType":"modelInstanceVersion","modelInstanceId":4629},{"sourceId":6103,"sourceType":"modelInstanceVersion","modelInstanceId":4646},{"sourceId":6113,"sourceType":"modelInstanceVersion","modelInstanceId":4618},{"sourceId":6127,"sourceType":"modelInstanceVersion","modelInstanceId":4598}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":3846.080383,"end_time":"2024-01-14T04:20:19.064569","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-14T03:16:12.984186","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"08983a9c6aff42578980f4f7113c3ee2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4411aefc021d46d0ada7b645eb53ec48","placeholder":"​","style":"IPY_MODEL_09a10a8cf9334c51857397ed50398c8e","value":"Searching best thr : 100%"}},"09a10a8cf9334c51857397ed50398c8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f3989a0c01248328e16875075e9d1c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08983a9c6aff42578980f4f7113c3ee2","IPY_MODEL_22cfcc0a7cc6455fbf3bb7c788c8a4e1","IPY_MODEL_c8392e8075224e3b8a020a16c1a08447"],"layout":"IPY_MODEL_6cec9a2c2fac450d87248aed8dd62f86"}},"22cfcc0a7cc6455fbf3bb7c788c8a4e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dffe80502d954bdea0bbb6353dbf5515","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ce1b34a4f864a42a6619eec82311eb0","value":20}},"4411aefc021d46d0ada7b645eb53ec48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cec9a2c2fac450d87248aed8dd62f86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce1b34a4f864a42a6619eec82311eb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83fe40a0b8f047cc8602206909d42361":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9384babdb7054d55aecdf3e989ddc926":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8392e8075224e3b8a020a16c1a08447":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83fe40a0b8f047cc8602206909d42361","placeholder":"​","style":"IPY_MODEL_9384babdb7054d55aecdf3e989ddc926","value":" 20/20 [04:34&lt;00:00, 12.66s/it]"}},"dffe80502d954bdea0bbb6353dbf5515":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HMS - Harmful Brain Activity Classification with [KerasCV](https://github.com/keras-team/keras-cv) and [Keras](https://github.com/keras-team/keras)\n\n> The objective of this competition is to classify seizures and other patterns of harmful brain activity in critically ill patients\n\nThis notebook guides you through the process of training and inferring a Deep Learning model, specifically EfficientNetV2, using KerasCV on the competition dataset. Specificaclly, this notebook uses spectrogram of the eeg data to classify the patterns.\n\nFun fact: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. Utilizing KerasCV and Keras allows us to choose our preferred backend. Explore more details on [Keras](https://keras.io/keras_core/announcement/).\n\nIn this notebook, you will learn:\n\n* Loading the data efficiently using [`tf.data`](https://www.tensorflow.org/guide/data).\n* Creating the model using KerasCV presets.\n* Training the model.\n* Inference and Submission on test data.\n\n**Note**: For a more in-depth understanding of KerasCV, refer to the [KerasCV guides](https://keras.io/guides/keras_cv/).","metadata":{}},{"cell_type":"markdown","source":"# 🛠 | Install Libraries  \n\nSince internet access is **disabled** during inference, we cannot install libraries in the usual `!pip install <lib_name>` manner. Instead, we need to install libraries from local files. In the following cell, we will install libraries from our local files. The installation code stays very similar - we just use the `filepath` instead of the `filename` of the library. So now the code is `!pip install <local_filepath>`. \n\n> The `filepath` of these local libraries look quite complicated, but don't be intimidated! Also `--no-deps` argument ensures that we are not installing any additional libraries.","metadata":{"papermill":{"duration":0.011416,"end_time":"2024-01-14T03:16:16.470167","exception":false,"start_time":"2024-01-14T03:16:16.458751","status":"completed"},"tags":[]}},{"cell_type":"code","source":" !pip install -q /kaggle/input/kerasv3-lib-ds/keras_cv-0.8.2-py3-none-any.whl --no-deps\n!pip install -q /kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n!pip install -q /kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl --no-deps","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:10:23.499116Z","iopub.execute_input":"2024-07-17T09:10:23.500127Z","iopub.status.idle":"2024-07-17T09:11:05.669070Z","shell.execute_reply.started":"2024-07-17T09:10:23.500075Z","shell.execute_reply":"2024-07-17T09:11:05.668025Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 📚 | Import Libraries ","metadata":{"papermill":{"duration":0.010878,"end_time":"2024-01-14T03:17:49.510159","exception":false,"start_time":"2024-01-14T03:17:49.499281","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n\nimport keras_cv\nimport keras\nfrom keras import ops\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport joblib\n\nimport matplotlib.pyplot as plt ","metadata":{"papermill":{"duration":10.671979,"end_time":"2024-01-14T03:18:00.193134","exception":false,"start_time":"2024-01-14T03:17:49.521155","status":"completed"},"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-17T09:11:05.670791Z","iopub.execute_input":"2024-07-17T09:11:05.671204Z","iopub.status.idle":"2024-07-17T09:11:40.439958Z","shell.execute_reply.started":"2024-07-17T09:11:05.671170Z","shell.execute_reply":"2024-07-17T09:11:40.439089Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-17 09:11:31.103498: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-17 09:11:31.103565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-17 09:11:31.105641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"code","source":"#this code is for model of wavenet and yolo_v8.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # you can also use tensorflow or torch\n\nimport keras_cv\nimport keras\nfrom keras import ops\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport joblib\n\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.441024Z","iopub.execute_input":"2024-07-17T09:11:40.441678Z","iopub.status.idle":"2024-07-17T09:11:40.446478Z","shell.execute_reply.started":"2024-07-17T09:11:40.441646Z","shell.execute_reply":"2024-07-17T09:11:40.445746Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Library Versions","metadata":{"papermill":{"duration":0.010958,"end_time":"2024-01-14T03:18:00.215704","exception":false,"start_time":"2024-01-14T03:18:00.204746","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasCV:\", keras_cv.__version__)","metadata":{"papermill":{"duration":0.019435,"end_time":"2024-01-14T03:18:00.246368","exception":false,"start_time":"2024-01-14T03:18:00.226933","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:40.448314Z","iopub.execute_input":"2024-07-17T09:11:40.448585Z","iopub.status.idle":"2024-07-17T09:11:40.459645Z","shell.execute_reply.started":"2024-07-17T09:11:40.448559Z","shell.execute_reply":"2024-07-17T09:11:40.458903Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"TensorFlow: 2.15.0\nKeras: 3.0.4\nKerasCV: 0.8.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ⚙️ | Configuration","metadata":{"papermill":{"duration":0.010922,"end_time":"2024-01-14T03:18:00.26855","exception":false,"start_time":"2024-01-14T03:18:00.257628","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# class CFG:\n#     verbose = 1  # Verbosity\n#     seed = 42  # Random seed\n#     preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n#     image_size = [400, 300]  # Input image size\n#     epochs = 13 # Training epochs\n#     batch_size = 64  # Batch size\n#     lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n#     drop_remainder = True  # Drop incomplete batches\n#     num_classes = 6 # Number of classes in the dataset\n#     fold = 0 # Which fold to set as validation data\n#     class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n#     label2name = dict(enumerate(class_names))\n#     name2label = {v:k for k, v in label2name.items()}","metadata":{"papermill":{"duration":0.018795,"end_time":"2024-01-14T03:18:00.298534","exception":false,"start_time":"2024-01-14T03:18:00.279739","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:40.460608Z","iopub.execute_input":"2024-07-17T09:11:40.460863Z","iopub.status.idle":"2024-07-17T09:11:40.469478Z","shell.execute_reply.started":"2024-07-17T09:11:40.460837Z","shell.execute_reply":"2024-07-17T09:11:40.468755Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     verbose = 1  # Verbosity\n#     seed = 42  # Random seed\n#     preset = \"resnet50_v2_imagenet\"  # Name of pretrained classifier\n#     image_size = [400, 300]  # Input image size\n#     epochs = 13 # Training epochs\n#     batch_size = 32  # Batch size\n#     lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n#     drop_remainder = True  # Drop incomplete batches\n#     num_classes = 6 # Number of classes in the dataset\n#     fold = 0 # Which fold to set as validation data\n#     class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n#     label2name = dict(enumerate(class_names))\n#     name2label = {v:k for k, v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.470442Z","iopub.execute_input":"2024-07-17T09:11:40.471049Z","iopub.status.idle":"2024-07-17T09:11:40.478976Z","shell.execute_reply.started":"2024-07-17T09:11:40.471020Z","shell.execute_reply":"2024-07-17T09:11:40.478250Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     verbose = 1  # Verbosity\n#     seed = 42  # Random seed\n#     preset = \"mobilenet_v3_large_imagenet\"  # Name of pretrained classifier\n#     image_size = [400, 300]  # Input image size\n#     epochs = 13 # Training epochs\n#     batch_size = 32  # Batch size\n#     lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n#     drop_remainder = True  # Drop incomplete batches\n#     num_classes = 6 # Number of classes in the dataset\n#     fold = 0 # Which fold to set as validation data\n#     class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n#     label2name = dict(enumerate(class_names))\n#     name2label = {v:k for k, v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.480056Z","iopub.execute_input":"2024-07-17T09:11:40.480601Z","iopub.status.idle":"2024-07-17T09:11:40.491363Z","shell.execute_reply.started":"2024-07-17T09:11:40.480571Z","shell.execute_reply":"2024-07-17T09:11:40.490552Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     verbose = 1  # Verbosity\n#     seed = 42  # Random seed\n#     preset = \"densenet201_imagenet\"  # Name of pretrained classifier\n#     image_size = [400, 300]  # Input image size\n#     epochs = 13 # Training epochs\n#     batch_size = 32  # Batch size\n#     lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n#     drop_remainder = True  # Drop incomplete batches\n#     num_classes = 6 # Number of classes in the dataset\n#     fold = 0 # Which fold to set as validation data\n#     class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n#     label2name = dict(enumerate(class_names))\n#     name2label = {v:k for k, v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.492239Z","iopub.execute_input":"2024-07-17T09:11:40.492493Z","iopub.status.idle":"2024-07-17T09:11:40.500819Z","shell.execute_reply.started":"2024-07-17T09:11:40.492470Z","shell.execute_reply":"2024-07-17T09:11:40.500128Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     verbose = 1  # Verbosity\n#     seed = 42  # Random seed\n#     preset = \"densenet201_imagenet\"  # Name of pretrained classifier\n#     image_size = [400, 300]  # Input image size\n#     epochs = 13 # Training epochs\n#     batch_size = 32  # Batch size\n#     lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n#     drop_remainder = True  # Drop incomplete batches\n#     num_classes = 6 # Number of classes in the dataset\n#     fold = 0 # Which fold to set as validation data\n#     class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n#     label2name = dict(enumerate(class_names))\n#     name2label = {v:k for k, v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.501770Z","iopub.execute_input":"2024-07-17T09:11:40.502008Z","iopub.status.idle":"2024-07-17T09:11:40.514059Z","shell.execute_reply.started":"2024-07-17T09:11:40.501985Z","shell.execute_reply":"2024-07-17T09:11:40.513257Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    verbose = 1  # Verbosity\n    seed = 42  # Random seed\n    preset = \"yolo_v8_l_backbone\"  # Name of pretrained classifier\n    image_size = [400, 300]  # Input image size\n    epochs = 13 # Training epochs\n    batch_size = 32  # Batch size\n    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n    drop_remainder = True  # Drop incomplete batches\n    num_classes = 6 # Number of classes in the dataset\n    fold = 0 # Which fold to set as validation data\n    class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n    label2name = dict(enumerate(class_names))\n    name2label = {v:k for k, v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.515077Z","iopub.execute_input":"2024-07-17T09:11:40.515356Z","iopub.status.idle":"2024-07-17T09:11:40.524154Z","shell.execute_reply.started":"2024-07-17T09:11:40.515329Z","shell.execute_reply":"2024-07-17T09:11:40.523371Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#this code is for model of wavenet and yolo_v8.\nclass CFG:\n    verbose = 1  # Verbosity\n    seed = 42  # Random seed\n    preset = \"yolo_v8_l_backbone\"  # Name of pretrained classifier\n    image_size = [400, 300]  # Input image size\n    epochs = 13  # Training epochs\n    batch_size = 32  # Batch size\n    lr_mode = \"cos\"  # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n    drop_remainder = True  # Drop incomplete batches\n    num_classes = 6  # Number of classes in the dataset\n    fold = 0  # Which fold to set as validation data\n    class_names = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']\n    label2name = dict(enumerate(class_names))\n    name2label = {v: k for k, v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.527488Z","iopub.execute_input":"2024-07-17T09:11:40.527876Z","iopub.status.idle":"2024-07-17T09:11:40.534987Z","shell.execute_reply.started":"2024-07-17T09:11:40.527842Z","shell.execute_reply":"2024-07-17T09:11:40.534216Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# ♻️ | Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{"papermill":{"duration":0.010907,"end_time":"2024-01-14T03:18:00.32063","exception":false,"start_time":"2024-01-14T03:18:00.309723","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"papermill":{"duration":0.018371,"end_time":"2024-01-14T03:18:00.350074","exception":false,"start_time":"2024-01-14T03:18:00.331703","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:40.535975Z","iopub.execute_input":"2024-07-17T09:11:40.536202Z","iopub.status.idle":"2024-07-17T09:11:40.547300Z","shell.execute_reply.started":"2024-07-17T09:11:40.536179Z","shell.execute_reply":"2024-07-17T09:11:40.546571Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 📁 | Dataset Path ","metadata":{"papermill":{"duration":0.010888,"end_time":"2024-01-14T03:18:00.372053","exception":false,"start_time":"2024-01-14T03:18:00.361165","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/hms-harmful-brain-activity-classification\"\n\nSPEC_DIR = \"/tmp/dataset/hms-hbac\"\nos.makedirs(SPEC_DIR+'/train_spectrograms', exist_ok=True)\nos.makedirs(SPEC_DIR+'/test_spectrograms', exist_ok=True)","metadata":{"papermill":{"duration":0.017704,"end_time":"2024-01-14T03:18:00.400852","exception":false,"start_time":"2024-01-14T03:18:00.383148","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:40.548257Z","iopub.execute_input":"2024-07-17T09:11:40.548495Z","iopub.status.idle":"2024-07-17T09:11:40.557922Z","shell.execute_reply.started":"2024-07-17T09:11:40.548472Z","shell.execute_reply":"2024-07-17T09:11:40.557225Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 📖 | Meta Data ","metadata":{"papermill":{"duration":0.011434,"end_time":"2024-01-14T03:18:00.472401","exception":false,"start_time":"2024-01-14T03:18:00.460967","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train + Valid\ndf = pd.read_csv(f'{BASE_PATH}/train.csv')\ndf['eeg_path'] = f'{BASE_PATH}/train_eegs/'+df['eeg_id'].astype(str)+'.parquet'\ndf['spec_path'] = f'{BASE_PATH}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.parquet'\ndf['spec2_path'] = f'{SPEC_DIR}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.npy'\ndf['class_name'] = df.expert_consensus.copy()\ndf['class_label'] = df.expert_consensus.map(CFG.name2label)\ndisplay(df.head(2))\n\n# Test\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\ntest_df['eeg_path'] = f'{BASE_PATH}/test_eegs/'+test_df['eeg_id'].astype(str)+'.parquet'\ntest_df['spec_path'] = f'{BASE_PATH}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.parquet'\ntest_df['spec2_path'] = f'{SPEC_DIR}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.npy'\ndisplay(test_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:40.558844Z","iopub.execute_input":"2024-07-17T09:11:40.559105Z","iopub.status.idle":"2024-07-17T09:11:41.084034Z","shell.execute_reply.started":"2024-07-17T09:11:40.559079Z","shell.execute_reply":"2024-07-17T09:11:41.083115Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"       eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n0  1628180742           0                       0.0          353733   \n1  1628180742           1                       6.0          353733   \n\n   spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n0                   0                               0.0   127492639   \n1                   1                               6.0  3887563113   \n\n   patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  lrda_vote  \\\n0       42516          Seizure             3         0         0          0   \n1       42516          Seizure             3         0         0          0   \n\n   grda_vote  other_vote                                           eeg_path  \\\n0          0           0  /kaggle/input/hms-harmful-brain-activity-class...   \n1          0           0  /kaggle/input/hms-harmful-brain-activity-class...   \n\n                                           spec_path  \\\n0  /kaggle/input/hms-harmful-brain-activity-class...   \n1  /kaggle/input/hms-harmful-brain-activity-class...   \n\n                                          spec2_path class_name  class_label  \n0  /tmp/dataset/hms-hbac/train_spectrograms/35373...    Seizure            0  \n1  /tmp/dataset/hms-hbac/train_spectrograms/35373...    Seizure            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eeg_id</th>\n      <th>eeg_sub_id</th>\n      <th>eeg_label_offset_seconds</th>\n      <th>spectrogram_id</th>\n      <th>spectrogram_sub_id</th>\n      <th>spectrogram_label_offset_seconds</th>\n      <th>label_id</th>\n      <th>patient_id</th>\n      <th>expert_consensus</th>\n      <th>seizure_vote</th>\n      <th>lpd_vote</th>\n      <th>gpd_vote</th>\n      <th>lrda_vote</th>\n      <th>grda_vote</th>\n      <th>other_vote</th>\n      <th>eeg_path</th>\n      <th>spec_path</th>\n      <th>spec2_path</th>\n      <th>class_name</th>\n      <th>class_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1628180742</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>353733</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>127492639</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/hms-harmful-brain-activity-class...</td>\n      <td>/kaggle/input/hms-harmful-brain-activity-class...</td>\n      <td>/tmp/dataset/hms-hbac/train_spectrograms/35373...</td>\n      <td>Seizure</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1628180742</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>353733</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>3887563113</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>/kaggle/input/hms-harmful-brain-activity-class...</td>\n      <td>/kaggle/input/hms-harmful-brain-activity-class...</td>\n      <td>/tmp/dataset/hms-hbac/train_spectrograms/35373...</td>\n      <td>Seizure</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   spectrogram_id      eeg_id  patient_id  \\\n0          853520  3911565283        6885   \n\n                                            eeg_path  \\\n0  /kaggle/input/hms-harmful-brain-activity-class...   \n\n                                           spec_path  \\\n0  /kaggle/input/hms-harmful-brain-activity-class...   \n\n                                          spec2_path  \n0  /tmp/dataset/hms-hbac/test_spectrograms/853520...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>spectrogram_id</th>\n      <th>eeg_id</th>\n      <th>patient_id</th>\n      <th>eeg_path</th>\n      <th>spec_path</th>\n      <th>spec2_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>853520</td>\n      <td>3911565283</td>\n      <td>6885</td>\n      <td>/kaggle/input/hms-harmful-brain-activity-class...</td>\n      <td>/kaggle/input/hms-harmful-brain-activity-class...</td>\n      <td>/tmp/dataset/hms-hbac/test_spectrograms/853520...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Convert `.parquet` to `.npy`\n\nTo facilitate easier data loading, we will convert the EEG spectrograms from `parquet` to `npy` format. This process involves saving the spectrogram data, and since the content of the files remains the same, no significant changes are made. \n\n> It's worth noting that the `time` column is excluded, as it is not part of the spectrogram.","metadata":{}},{"cell_type":"code","source":"# Define a function to process a single eeg_id\ndef process_spec(spec_id, split=\"train\"):\n    spec_path = f\"{BASE_PATH}/{split}_spectrograms/{spec_id}.parquet\"\n    spec = pd.read_parquet(spec_path)\n    spec = spec.fillna(0).values[:, 1:].T # fill NaN values with 0, transpose for (Time, Freq) -> (Freq, Time)\n    spec = spec.astype(\"float32\")\n    np.save(f\"{SPEC_DIR}/{split}_spectrograms/{spec_id}.npy\", spec)\n\n# Get unique spec_ids of train and valid data\nspec_ids = df[\"spectrogram_id\"].unique()\n\n# Parallelize the processing using joblib for training data\n_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n    joblib.delayed(process_spec)(spec_id, \"train\")\n    for spec_id in tqdm(spec_ids, total=len(spec_ids))\n)\n\n# Get unique spec_ids of test data\ntest_spec_ids = test_df[\"spectrogram_id\"].unique()\n\n# Parallelize the processing using joblib for test data\n_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n    joblib.delayed(process_spec)(spec_id, \"test\")\n    for spec_id in tqdm(test_spec_ids, total=len(test_spec_ids))\n)","metadata":{"papermill":{"duration":0.86264,"end_time":"2024-01-14T03:18:01.346487","exception":false,"start_time":"2024-01-14T03:18:00.483847","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.085519Z","iopub.execute_input":"2024-07-17T09:11:41.085884Z","iopub.status.idle":"2024-07-17T09:11:41.647303Z","shell.execute_reply.started":"2024-07-17T09:11:41.085850Z","shell.execute_reply":"2024-07-17T09:11:41.645601Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m spec_ids \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectrogram_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Parallelize the processing using joblib for training data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _ \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mParallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[1;32m     14\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdelayed(process_spec)(spec_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m spec_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspec_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get unique spec_ids of test data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m test_spec_ids \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectrogram_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n","\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"],"ename":"ImportError","evalue":"IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html","output_type":"error"}]},{"cell_type":"markdown","source":"# 🍚 | DataLoader\n\nThis DataLoader first reads `npy` spectrogram files and extracts labeled subsamples using specified `offset` values. Then, it converts the spectrogram data into `log spectrogram` and applies the popular signal augmentation `MixUp`.\n\n> Note that, we are converting the mono channel signal to a 3-channel signal for using \"ImageNet\" weights of pretrained model.","metadata":{"papermill":{"duration":0.011843,"end_time":"2024-01-14T03:18:01.457956","exception":false,"start_time":"2024-01-14T03:18:01.446113","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_augmenter(dim=CFG.image_size):\n    augmenters = [\n        keras_cv.layers.MixUp(alpha=2.0),\n        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0),\n                                     width_factor=(0.06, 0.1)), # freq-masking\n        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1),\n                                     width_factor=(1.0, 1.0)), # time-masking\n    ]\n    \n    def augment(img, label):\n        data = {\"images\":img, \"labels\":label}\n        for augmenter in augmenters:\n            if tf.random.uniform([]) < 0.5:\n                data = augmenter(data, training=True)\n        return data[\"images\"], data[\"labels\"]\n    \n    return augment\n\n\ndef build_decoder(with_labels=True, target_size=CFG.image_size, dtype=32):\n    def decode_signal(path, offset=None):\n        # Read .npy files and process the signal\n        file_bytes = tf.io.read_file(path)\n        sig = tf.io.decode_raw(file_bytes, tf.float32)\n        sig = sig[1024//dtype:]  # Remove header tag\n        sig = tf.reshape(sig, [400, -1])\n        \n        # Extract labeled subsample from full spectrogram using \"offset\"\n        if offset is not None: \n            offset = offset // 2  # Only odd values are given\n            sig = sig[:, offset:offset+300]\n            \n            # Pad spectrogram to ensure the same input shape of [400, 300]\n            pad_size = tf.math.maximum(0, 300 - tf.shape(sig)[1])\n            sig = tf.pad(sig, [[0, 0], [0, pad_size]])\n            sig = tf.reshape(sig, [400, 300])\n        \n        # Log spectrogram \n        sig = tf.clip_by_value(sig, tf.math.exp(-4.0), tf.math.exp(8.0)) # avoid 0 in log\n        sig = tf.math.log(sig)\n        \n        # Normalize spectrogram\n        sig -= tf.math.reduce_mean(sig)\n        sig /= tf.math.reduce_std(sig) + 1e-6\n        \n        # Mono channel to 3 channels to use \"ImageNet\" weights\n        sig = tf.tile(sig[..., None], [1, 1, 3])\n        return sig\n    \n    def decode_label(label):\n        label = tf.one_hot(label, CFG.num_classes)\n        label = tf.cast(label, tf.float32)\n        label = tf.reshape(label, [CFG.num_classes])\n        return label\n    \n    def decode_with_labels(path, offset=None, label=None):\n        sig = decode_signal(path, offset)\n        label = decode_label(label)\n        return (sig, label)\n    \n    return decode_with_labels if with_labels else decode_signal\n\n\ndef build_dataset(paths, offsets=None, labels=None, batch_size=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=False, repeat=True, shuffle=1024, \n                  cache_dir=\"\", drop_remainder=False):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter()\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = (paths, offsets) if labels is None else (paths, offsets, labels)\n    \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    ds = ds.cache(cache_dir) if cache else ds\n    ds = ds.repeat() if repeat else ds\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.prefetch(AUTO)\n    return ds","metadata":{"papermill":{"duration":0.039133,"end_time":"2024-01-14T03:18:01.509017","exception":false,"start_time":"2024-01-14T03:18:01.469884","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.648130Z","iopub.status.idle":"2024-07-17T09:11:41.648447Z","shell.execute_reply.started":"2024-07-17T09:11:41.648295Z","shell.execute_reply":"2024-07-17T09:11:41.648311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔪 | Data Split\n\nIn the following code snippet, the data is divided into `5` folds. Note that, the `groups` argument is used to prevent any overlap of patients between the training and validation sets, thus avoiding potential **data leakage** issues. Additionally, each split is stratified based on the `class_label`, ensuring a uniform distribution of class labels in each fold.","metadata":{"papermill":{"duration":0.012174,"end_time":"2024-01-14T03:18:01.538524","exception":false,"start_time":"2024-01-14T03:18:01.52635","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold\n\nsgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n\ndf[\"fold\"] = -1\ndf.reset_index(drop=True, inplace=True)\nfor fold, (train_idx, valid_idx) in enumerate(\n    sgkf.split(df, y=df[\"class_label\"], groups=df[\"patient_id\"])\n):\n    df.loc[valid_idx, \"fold\"] = fold\ndf.groupby([\"fold\", \"class_name\"])[[\"eeg_id\"]].count().T","metadata":{"papermill":{"duration":0.037496,"end_time":"2024-01-14T03:18:01.587924","exception":false,"start_time":"2024-01-14T03:18:01.550428","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.649815Z","iopub.status.idle":"2024-07-17T09:11:41.650112Z","shell.execute_reply.started":"2024-07-17T09:11:41.649971Z","shell.execute_reply":"2024-07-17T09:11:41.649985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df = df.copy()\nsample_df[\"sequence_id\"] = sample_df.groupby([\"eeg_id\"]).cumcount()\ntrain_df = sample_df.query(\"fold != @CFG.fold\").reset_index(drop=True)\nvalid_df = sample_df.query(\"fold == @CFG.fold\").reset_index(drop=True)\n\n# Train\ntrain_paths = train_df[\"spec2_path\"].values\ntrain_offsets = train_df[\"sequence_id\"].values * 2  # *2 to get \"even\" and \"odd\" window offsets\ntrain_labels = train_df[\"class_label\"].values\n\ntrain_ds = build_dataset(\n    train_paths, train_offsets, train_labels,\n    decode_fn=build_decoder(with_labels=True),\n    augment_fn=build_augmenter(),\n    augment=True, batch_size=CFG.batch_size, cache=True, shuffle=True, repeat=True,\n    drop_remainder=CFG.drop_remainder\n)\n\n# Valid\nvalid_paths = valid_df[\"spec2_path\"].values\nvalid_offsets = valid_df[\"sequence_id\"].values * 2  # *2 to get \"even\" and \"odd\" window offsets\nvalid_labels = valid_df[\"class_label\"].values\n\nvalid_ds = build_dataset(\n    valid_paths, valid_offsets, valid_labels,\n    decode_fn=build_decoder(with_labels=True),\n    augment_fn=build_augmenter(),\n    augment=False, batch_size=CFG.batch_size*2, cache=True, shuffle=False, repeat=True,\n    drop_remainder=CFG.drop_remainder\n)\n\n# Full data\ntrain_full_paths = sample_df[\"spec2_path\"].values\ntrain_full_offsets = sample_df[\"sequence_id\"].values * 2  # *2 to get \"even\" and \"odd\" window offsets\ntrain_full_labels = sample_df[\"class_label\"].values\n\ntrain_full_ds = build_dataset(\n    train_full_paths, train_full_offsets, train_full_labels,\n    decode_fn=build_decoder(with_labels=True),\n    augment_fn=build_augmenter(),\n    augment=True, batch_size=CFG.batch_size, cache=True, shuffle=True, repeat=True,\n    drop_remainder=CFG.drop_remainder\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.651063Z","iopub.status.idle":"2024-07-17T09:11:41.651336Z","shell.execute_reply.started":"2024-07-17T09:11:41.651200Z","shell.execute_reply":"2024-07-17T09:11:41.651214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Train & Valid Dataset\n\nOnly first sample for each `spectrogram_id` is used in order to keep the dataset size managable. Feel free to train on full data.","metadata":{"papermill":{"duration":0.011875,"end_time":"2024-01-14T03:18:01.611955","exception":false,"start_time":"2024-01-14T03:18:01.60008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Sample from full data\nsample_df = df.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\ntrain_df = sample_df[sample_df.fold != CFG.fold]\nvalid_df = sample_df[sample_df.fold == CFG.fold]\nprint(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n\n# Train\ntrain_paths = train_df.spec2_path.values\ntrain_offsets = train_df.spectrogram_label_offset_seconds.values.astype(int)\ntrain_labels = train_df.class_label.values\ntrain_ds = build_dataset(train_paths, train_offsets, train_labels, batch_size=CFG.batch_size,\n                         repeat=True, shuffle=True, augment=True, cache=True)\n\n# Valid\nvalid_paths = valid_df.spec2_path.values\nvalid_offsets = valid_df.spectrogram_label_offset_seconds.values.astype(int)\nvalid_labels = valid_df.class_label.values\nvalid_ds = build_dataset(valid_paths, valid_offsets, valid_labels, batch_size=CFG.batch_size,\n                         repeat=False, shuffle=False, augment=False, cache=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.651996Z","iopub.status.idle":"2024-07-17T09:11:41.652265Z","shell.execute_reply.started":"2024-07-17T09:11:41.652133Z","shell.execute_reply":"2024-07-17T09:11:41.652147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define YOLOv8 model\nbackbone = keras_cv.models.YOLOV8Backbone.from_preset(CFG.preset)\n\ninputs = keras.Input([*CFG.image_size, 3])\nyolo_features = backbone(inputs, training=True)[\"level5\"]\nyolo_features = keras.layers.GlobalAveragePooling2D()(yolo_features)\nyolo_features = keras.layers.Dropout(0.3)(yolo_features)\n\n# Define WaveNet model\nwave_inputs = keras.Input(shape=[400, 300, 3])\nwave_x = keras.layers.Conv1D(32, kernel_size=2, activation='relu')(wave_inputs)\nwave_x = keras.layers.MaxPooling1D(pool_size=2)(wave_x)\nwave_x = keras.layers.Conv1D(64, kernel_size=2, activation='relu')(wave_x)\nwave_x = keras.layers.MaxPooling1D(pool_size=2)(wave_x)\nwave_x = keras.layers.GlobalAveragePooling1D()(wave_x)\nwave_x = keras.layers.Dropout(0.3)(wave_x)\n\n# Combine YOLOv8 and WaveNet features\ncombined_features = keras.layers.Concatenate()([yolo_features, wave_x])\noutputs = keras.layers.Dense(CFG.num_classes, activation=\"softmax\")(combined_features)\n\nmodel = keras.Model(inputs=[inputs, wave_inputs], outputs=outputs)\nmodel.summary()\n\ntotal_steps = (len(train_paths) // CFG.batch_size) * CFG.epochs\nwarmup_steps = total_steps * 0.2\nwarmup_lr = CFG.lr * 0.1\n\nif CFG.lr_mode == \"cos\":\n    scheduler = keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=CFG.lr,\n        decay_steps=total_steps - warmup_steps,\n        alpha=0.1,\n    )\nelif CFG.lr_mode == \"exp\":\n    scheduler = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=CFG.lr,\n        decay_steps=total_steps - warmup_steps,\n        decay_rate=0.7,\n    )\nelif CFG.lr_mode == \"step\":\n    scheduler = keras.optimizers.schedules.PiecewiseConstantDecay(\n        boundaries=[total_steps * 0.5, total_steps * 0.8],\n        values=[CFG.lr, CFG.lr * 0.1, CFG.lr * 0.01],\n    )\n\ndef warmup(epoch, lr):\n    return warmup_lr + (scheduler(epoch - warmup_steps) if epoch >= warmup_steps else lr)\n\nwarmup_scheduler = keras.callbacks.LearningRateScheduler(warmup, verbose=CFG.verbose)\nearly_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1, restore_best_weights=True)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=CFG.lr),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.653585Z","iopub.status.idle":"2024-07-17T09:11:41.653921Z","shell.execute_reply.started":"2024-07-17T09:11:41.653744Z","shell.execute_reply":"2024-07-17T09:11:41.653758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    [train_ds, train_ds],\n    epochs=CFG.epochs,\n    steps_per_epoch=len(train_paths) // CFG.batch_size,\n    validation_data=([valid_ds, valid_ds]),\n    validation_steps=len(valid_paths) // (CFG.batch_size * 2),\n    callbacks=[warmup_scheduler, early_stopping],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.655247Z","iopub.status.idle":"2024-07-17T09:11:41.655587Z","shell.execute_reply.started":"2024-07-17T09:11:41.655412Z","shell.execute_reply":"2024-07-17T09:11:41.655429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\nax1.plot(history.history['accuracy'], label='train accuracy')\nax1.plot(history.history['val_accuracy'], label='val accuracy')\nax1.set_title('Model accuracy')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Accuracy')\nax1.legend(loc='upper left')\n\nax2.plot(history.history['loss'], label='train loss')\nax2.plot(history.history['val_loss'], label='val loss')\nax2.set_title('Model loss')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\nax2.legend(loc='upper left')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.656315Z","iopub.status.idle":"2024-07-17T09:11:41.656585Z","shell.execute_reply.started":"2024-07-17T09:11:41.656454Z","shell.execute_reply":"2024-07-17T09:11:41.656467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Check\n\nLet's visualize some samples from the dataset.","metadata":{}},{"cell_type":"code","source":"imgs, tars = next(iter(train_ds))\n\nnum_imgs = 8\nplt.figure(figsize=(4*4, num_imgs//4*5))\nfor i in range(num_imgs):\n    plt.subplot(num_imgs//4, 4, i + 1)\n    img = imgs[i].numpy()[...,0]  # Adjust as per your image data format\n    img -= img.min()\n    img /= img.max() + 1e-4\n    tar = CFG.label2name[np.argmax(tars[i].numpy())]\n    plt.imshow(img)\n    plt.title(f\"Target: {tar}\")\n    plt.axis('off')\n    \nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-17T09:11:41.657595Z","iopub.status.idle":"2024-07-17T09:11:41.657909Z","shell.execute_reply.started":"2024-07-17T09:11:41.657758Z","shell.execute_reply":"2024-07-17T09:11:41.657773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.658971Z","iopub.status.idle":"2024-07-17T09:11:41.659263Z","shell.execute_reply.started":"2024-07-17T09:11:41.659110Z","shell.execute_reply":"2024-07-17T09:11:41.659124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parquet_file = \"/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/1000913311.parquet\" ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.659885Z","iopub.status.idle":"2024-07-17T09:11:41.660158Z","shell.execute_reply.started":"2024-07-17T09:11:41.660018Z","shell.execute_reply":"2024-07-17T09:11:41.660032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_parquet(parquet_file,engine='auto')","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.661225Z","iopub.status.idle":"2024-07-17T09:11:41.661517Z","shell.execute_reply.started":"2024-07-17T09:11:41.661368Z","shell.execute_reply":"2024-07-17T09:11:41.661382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔍 | Loss & Metric\n\nThe evaluation metric in this competition is **KL Divergence**, defined as,\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right)\n$$\n\nWhere:\n- $P$ is the true distribution.\n- $Q$ is the predicted distribution.\n\nInterestingly, as KL Divergence is differentiable, we can directly use it as our loss function. Thus, we don't need to use a third-party metric like **Accuracy** to evaluate our model. Therefore, `valid_loss` can stand alone as an indicator for our evaluation. In keras, we already have impelementation for KL Divergence loss so we only need to import it.","metadata":{}},{"cell_type":"code","source":"LOSS = keras.losses.KLDivergence()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.662475Z","iopub.status.idle":"2024-07-17T09:11:41.662781Z","shell.execute_reply.started":"2024-07-17T09:11:41.662620Z","shell.execute_reply":"2024-07-17T09:11:41.662634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🤖 | Modeling\n\nThis notebook uses the `EfficientNetV2 B2` from KerasCV's collection of pretrained models. To explore other models, simply modify the `preset` in the `CFG` (config). Check the [KerasCV website](https://keras.io/api/keras_cv/models/tasks/image_classifier/) for a list of available pretrained models.","metadata":{"papermill":{"duration":0.016849,"end_time":"2024-01-14T03:18:38.613991","exception":false,"start_time":"2024-01-14T03:18:38.597142","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build Classifier\nmodel = keras_cv.models.ImageClassifier.from_preset(\n    CFG.preset, num_classes=CFG.num_classes\n)\n\n# Compile the model  \nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n              loss=LOSS)\n\n# Model Sumamry\nmodel.summary()","metadata":{"papermill":{"duration":10.446166,"end_time":"2024-01-14T03:18:49.186176","exception":false,"start_time":"2024-01-14T03:18:38.74001","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.664133Z","iopub.status.idle":"2024-07-17T09:11:41.664473Z","shell.execute_reply.started":"2024-07-17T09:11:41.664292Z","shell.execute_reply":"2024-07-17T09:11:41.664309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ⚓ | LR Schedule\n\nA well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.","metadata":{"papermill":{"duration":0.016209,"end_time":"2024-01-14T03:18:49.21924","exception":false,"start_time":"2024-01-14T03:18:49.203031","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 5e-5, 6e-6 * batch_size, 1e-5\n    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"papermill":{"duration":0.028945,"end_time":"2024-01-14T03:18:49.264535","exception":false,"start_time":"2024-01-14T03:18:49.23559","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.665452Z","iopub.status.idle":"2024-07-17T09:11:41.665808Z","shell.execute_reply.started":"2024-07-17T09:11:41.665621Z","shell.execute_reply":"2024-07-17T09:11:41.665637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)","metadata":{"papermill":{"duration":0.297147,"end_time":"2024-01-14T03:18:49.578089","exception":false,"start_time":"2024-01-14T03:18:49.280942","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.666719Z","iopub.status.idle":"2024-07-17T09:11:41.667005Z","shell.execute_reply.started":"2024-07-17T09:11:41.666860Z","shell.execute_reply":"2024-07-17T09:11:41.666879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 💾 | Model Checkpointing","metadata":{"papermill":{"duration":0.017199,"end_time":"2024-01-14T03:18:49.613648","exception":false,"start_time":"2024-01-14T03:18:49.596449","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(\"best_model.keras\",\n                                         monitor='val_loss',\n                                         save_best_only=True,\n                                         save_weights_only=False,\n                                         mode='min')","metadata":{"papermill":{"duration":0.024529,"end_time":"2024-01-14T03:18:49.655708","exception":false,"start_time":"2024-01-14T03:18:49.631179","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.667850Z","iopub.status.idle":"2024-07-17T09:11:41.668128Z","shell.execute_reply.started":"2024-07-17T09:11:41.667994Z","shell.execute_reply":"2024-07-17T09:11:41.668007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🚂 | Training","metadata":{"papermill":{"duration":0.01671,"end_time":"2024-01-14T03:18:49.689354","exception":false,"start_time":"2024-01-14T03:18:49.672644","status":"completed"},"tags":[]}},{"cell_type":"code","source":"VERBOSE = 1\nFOLDS_TO_TRAIN = 5\nif not os.path.exists('WaveNet_Model'):\n    os.makedirs('WaveNet_Model')\n\nfrom sklearn.model_selection import KFold, GroupKFold\nimport tensorflow.keras.backend as K, gc\n\nall_oof = []; all_oof2 = []; all_true = []\ngkf = GroupKFold(n_splits=5)\nfor i, (train_index, valid_index) in enumerate(gkf.split(train, train.target, train.patient_id)):   \n    \n    print('#'*25)\n    print(f'### Fold {i+1}')\n    train_gen = DataGenerator(train.iloc[train_index], shuffle=True, batch_size=32)\n    valid_gen = DataGenerator(train.iloc[valid_index], shuffle=False, batch_size=64, mode='valid')\n    print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n    print('#'*25)\n    \n    # TRAIN MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model()\n    if TRAIN_MODEL:\n        model.fit(train_gen, verbose=VERBOSE,\n              validation_data = valid_gen,\n              epochs=EPOCHS, callbacks = [LR])\n        model.save_weights(f'WaveNet_Model/WaveNet_fold{i}.h5')\n    else:\n        model.load_weights(f'/kaggle/input/brain-eegs/WaveNet_Model/WaveNet_fold{i}.h5')\n    \n    # WAVENET OOF\n    oof = model.predict(valid_gen, verbose=VERBOSE)\n    all_oof.append(oof)\n    all_true.append(train.iloc[valid_index][TARGETS].values)\n    \n    # TRAIN MEAN OOF\n    y_train = train.iloc[train_index][TARGETS].values\n    y_valid = train.iloc[valid_index][TARGETS].values\n    oof = y_valid.copy()\n    for j in range(6):\n        oof[:,j] = y_train[:,j].mean()\n    oof = oof / oof.sum(axis=1,keepdims=True)\n    all_oof2.append(oof)\n    \n    del model, oof, y_train, y_valid\n    gc.collect()\n    \n    if i==FOLDS_TO_TRAIN-1: break\n    \nall_oof = np.concatenate(all_oof)\nall_oof2 = np.concatenate(all_oof2)\nall_true = np.concatenate(all_true)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.668783Z","iopub.status.idle":"2024-07-17T09:11:41.669059Z","shell.execute_reply.started":"2024-07-17T09:11:41.668925Z","shell.execute_reply":"2024-07-17T09:11:41.668939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_ds, \n    epochs=100,\n    callbacks=[ckpt_cb], \n    steps_per_epoch=len(train_df)//CFG.batch_size,\n    validation_data=valid_ds, \n    verbose=CFG.verbose\n)","metadata":{"papermill":{"duration":3374.692199,"end_time":"2024-01-14T04:15:04.398389","exception":false,"start_time":"2024-01-14T03:18:49.70619","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.669899Z","iopub.status.idle":"2024-07-17T09:11:41.670183Z","shell.execute_reply.started":"2024-07-17T09:11:41.670043Z","shell.execute_reply":"2024-07-17T09:11:41.670057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\npreds = model.predict(valid_ds)\ny_pred = np.argmax(preds, axis=1)\nprint(classification_report(valid_labels,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.671337Z","iopub.status.idle":"2024-07-17T09:11:41.671621Z","shell.execute_reply.started":"2024-07-17T09:11:41.671483Z","shell.execute_reply":"2024-07-17T09:11:41.671497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🧪 | Prediction","metadata":{"papermill":{"duration":0.693309,"end_time":"2024-01-14T04:15:05.731839","exception":false,"start_time":"2024-01-14T04:15:05.03853","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Load Best Model","metadata":{"papermill":{"duration":0.632183,"end_time":"2024-01-14T04:15:06.991143","exception":false,"start_time":"2024-01-14T04:15:06.35896","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.load_weights(\"best_model.keras\")","metadata":{"papermill":{"duration":20.428261,"end_time":"2024-01-14T04:15:28.044401","exception":false,"start_time":"2024-01-14T04:15:07.61614","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-17T09:11:41.672451Z","iopub.status.idle":"2024-07-17T09:11:41.672753Z","shell.execute_reply.started":"2024-07-17T09:11:41.672587Z","shell.execute_reply":"2024-07-17T09:11:41.672601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Test Dataset","metadata":{"papermill":{"duration":0.703901,"end_time":"2024-01-14T04:20:09.745279","exception":false,"start_time":"2024-01-14T04:20:09.041378","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_paths = test_df.spec2_path.values\ntest_ds = build_dataset(test_paths, batch_size=min(CFG.batch_size, len(test_df)),\n                         repeat=False, shuffle=False, cache=False, augment=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.673419Z","iopub.status.idle":"2024-07-17T09:11:41.673688Z","shell.execute_reply.started":"2024-07-17T09:11:41.673556Z","shell.execute_reply":"2024-07-17T09:11:41.673570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"preds = model.predict(test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.675062Z","iopub.status.idle":"2024-07-17T09:11:41.675394Z","shell.execute_reply.started":"2024-07-17T09:11:41.675225Z","shell.execute_reply":"2024-07-17T09:11:41.675241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code the accuracy,precision,recall,F1 score.\nfrom sklearn.metrics import classification_report\npreds = model.predict(valid_ds)\ny_pred = np.argmax(preds, axis=1)\nprint(classification_report(valid_labels,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.676578Z","iopub.status.idle":"2024-07-17T09:11:41.676901Z","shell.execute_reply.started":"2024-07-17T09:11:41.676740Z","shell.execute_reply":"2024-07-17T09:11:41.676754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📩 | Submission","metadata":{}},{"cell_type":"code","source":"pred_df = test_df[[\"eeg_id\"]].copy()\ntarget_cols = [x.lower()+'_vote' for x in CFG.class_names]\npred_df[target_cols] = preds.tolist()\n\nsub_df = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\nsub_df = sub_df[[\"eeg_id\"]].copy()\nsub_df = sub_df.merge(pred_df, on=\"eeg_id\", how=\"left\")\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:41.678204Z","iopub.status.idle":"2024-07-17T09:11:41.678518Z","shell.execute_reply.started":"2024-07-17T09:11:41.678357Z","shell.execute_reply":"2024-07-17T09:11:41.678372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📌 | Reference\n* [HMS-HBAC: ResNet34d Baseline [Training]](https://www.kaggle.com/code/ttahara/hms-hbac-resnet34d-baseline-training) \n* [EfficientNetB2 Starter - [LB 0.57]](https://www.kaggle.com/code/cdeotte/efficientnetb2-starter-lb-0-57)","metadata":{}}]}